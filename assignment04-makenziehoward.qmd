---
title: Assignment 04
author:
  - name: Makenzie Howard
    affiliations:
      - id: bu
        name: Boston University
        city: Boston
        state: MA
number-sections: true
date: '2025-10-06'
date-modified: today
date-format: long
format:
  html:
    theme: cerulean
    toc: true
    toc-depth: 2

execute:
  echo: true
  eval: true
  freeze: auto
---

# Load the dataset
```{python}
#echo: true
#eval: true
from pyspark.sql import SparkSession
import pandas as pd
import plotly.express as px
import plotly.io as pio
import numpy as np

np.random.seed(42)

pio.renderers.default = "notebook+notebook_connected+vscode"

# Initialize Spark Session
spark = SparkSession.builder.appName("LightcastData").getOrCreate()

# Load Data
df = spark.read.option("header", "true").option("inferSchema", "true").option("multiLine","true").option("escape", "\"").csv("./data/lightcast_job_postings.csv")

# Show Schema and Sample Data
print("---This is Diagnostic check, No need to print it in the final doc---")

# df.printSchema() # comment this line when rendering the submission
df.show(5)

```

# Feature Engineering
```{python}
#| eval: true
#| echo: true
#| fig-align: center

from pyspark.sql.functions import col, pow  # pow not used here; safe to keep
from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler
from pyspark.ml import Pipeline

# Columns to preview for quick EDA
eda_cols = [
    "SALARY",
    "MIN_YEARS_EXPERIENCE", "DURATION",
    "COMPANY_IS_STAFFING", "IS_INTERNSHIP",
    "STATE_NAME", "REMOTE_TYPE_NAME", "EMPLOYMENT_TYPE_NAME",
    "MIN_EDULEVELS_NAME"
]

# Keep only the columns that actually exist; warn about any missing
existing_cols = [c for c in eda_cols if c in df.columns]
missing_cols  = [c for c in eda_cols if c not in df.columns]
if missing_cols:
    print("⚠️ Missing columns (skipped):", missing_cols)

# Select and show a few rows
df_eda = df.select(*existing_cols)
print("Showing columns:", existing_cols)
df_eda.show(5, truncate=False)


```

# Empty columns plot
```{python}
# Missingness by column (nulls + blank strings) + Plotly bar chart

from pyspark.sql.functions import col, sum as spark_sum, when, trim, length
import pandas as pd
import plotly.express as px

# Use your EDA subset if it exists; otherwise use the full df
src = df_eda if 'df_eda' in globals() else df

# 1) Spark: count null OR blank-string cells per column
null_or_blank_sums = [
    spark_sum(
        when(col(c).isNull() | (length(trim(col(c).cast("string"))) == 0), 1).otherwise(0)
    ).alias(c)
    for c in src.columns
]
missing_df = src.select(null_or_blank_sums)

# 2) To pandas + percentages
missing_pd = missing_df.toPandas().T.reset_index()
missing_pd.columns = ["column", "missing_count"]
total_rows = src.count()
missing_pd["missing_pct"] = 100.0 * missing_pd["missing_count"] / max(total_rows, 1)
missing_pd = missing_pd.sort_values("missing_pct", ascending=False)

# 3) Plot
fig = px.bar(
    missing_pd,
    x="column",
    y="missing_pct",
    title="Percentage of Missing Values by Column",
    labels={"column": "Features", "missing_pct": "Missing (%)"}
)
fig.update_layout(xaxis_tickangle=-45, height=500, width=900)
fig


```

# Unique values
```{python}
from pyspark.sql.functions import countDistinct

# use df_eda if it exists; else fall back to df
src = df_eda if 'df_eda' in globals() else df

src.select([countDistinct(c).alias(f"{c}_nunique") for c in src.columns]).show(truncate=False)


```

# Category counter
```{python}
from pyspark.sql.functions import count, desc

# Use your EDA subset if available
src = df_eda if 'df_eda' in globals() else df

categorical_cols = [
    "STATE_NAME",
    "REMOTE_TYPE_NAME",
    "EMPLOYMENT_TYPE_NAME",
    "MIN_EDULEVELS_NAME",
    "COMPANY_IS_STAFFING",
    "IS_INTERNSHIP",
]

TOP_N = 50  # how many categories to show per column

for colname in categorical_cols:
    if colname not in src.columns:
        print(f"Skipping {colname} (column not found)")
        continue

    distinct_cnt = src.select(colname).distinct().count()
    print(f"\n--- {colname} (distinct={distinct_cnt}) ---")
    (
        src.groupBy(colname)
           .agg(count("*").alias("count"))
           .orderBy(desc("count"))
           .show(min(TOP_N, distinct_cnt), truncate=False)
    )


```

#  Remote Type Name Fix
```{python}
from pyspark.sql.functions import col, when, trim

# Start from df_eda if it exists; otherwise use df
base = df_eda if 'df_eda' in globals() else df

# Normalize REMOTE_TYPE_NAME values
df_eda = (
    base.withColumn(
        "REMOTE_TYPE_NAME",
        when(trim(col("REMOTE_TYPE_NAME")) == "Remote", "Remote")
        .when(trim(col("REMOTE_TYPE_NAME")) == "[None]", "Undefined")
        .when(trim(col("REMOTE_TYPE_NAME")) == "Not Remote", "On Premise")
        .when(trim(col("REMOTE_TYPE_NAME")) == "Hybrid Remote", "Hybrid")
        .when(col("REMOTE_TYPE_NAME").isNull(), "On Premise")
        .otherwise(col("REMOTE_TYPE_NAME"))
    )
)

# df_eda.createOrReplaceTempView("df_eda")  # ← uncomment if you want a SQL temp view

categorical_cols = [
   "REMOTE_TYPE_NAME",
    
]

for colname in categorical_cols:
    if colname not in df_eda.columns:
        print(f"⚠️ Skipping {colname} (column not found)")
        continue
    print(f"\n--- {colname} ---")
    df_eda.select(colname).distinct().show(50, truncate=False)


```

# Employment type fix
```{python}
# Normalize EMPLOYMENT_TYPE_NAME values and inspect them

from pyspark.sql.functions import col, when, trim, lower, regexp_replace, desc, count

# Start from df_eda if it exists; otherwise use df
base = df_eda if 'df_eda' in globals() else df

# A normalized helper (lowercased, trimmed, single-spaced) for matching
emp_norm = regexp_replace(lower(trim(col("EMPLOYMENT_TYPE_NAME"))), r"\s+", " ")

df_eda = (
    base.withColumn(
        "EMPLOYMENT_TYPE_NAME",
        when(col("EMPLOYMENT_TYPE_NAME").isNull(), "Fulltime")                                   # NULL → Fulltime
        .when(emp_norm.isin("part-time / full-time", "part time / full time", "part-time/full-time"),
              "Flexible")                                                                        # "Part-time / full-time" → Flexible
        .when(emp_norm.rlike(r"part[- ]?time\s*\((<=|≤)\s*32\s*hours\)"),
              "Parttime")                                                                        # "Part-time (<=/≤ 32 hours)" → Parttime
        .when(emp_norm.rlike(r"full[- ]?time\s*\(>\s*32\s*hours\)"),
              "Fulltime")                                                                        # "Full-time (> 32 hours)" → Fulltime
        .otherwise(col("EMPLOYMENT_TYPE_NAME"))                                                  # keep others as-is
    )
)

# --- Option A: just distinct values (like your screenshot) ---
print("\n--- EMPLOYMENT_TYPE_NAME (distinct) ---")
df_eda.select("EMPLOYMENT_TYPE_NAME").distinct().show(50, truncate=False)

# --- Option B: frequency table (more informative) ---
print("\n--- EMPLOYMENT_TYPE_NAME (counts) ---")
(df_eda.groupBy("EMPLOYMENT_TYPE_NAME")
       .agg(count("*").alias("count"))
       .orderBy(desc("count"))
       .show(truncate=False))

```

YOU ARE HERE @ 2:07 IN VIDEO