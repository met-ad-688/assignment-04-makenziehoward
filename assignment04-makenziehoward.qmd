---
title: Assignment 04
author:
  - name: Makenzie Howard
    affiliations:
      - id: bu
        name: Boston University
        city: Boston
        state: MA
number-sections: true
date: '2025-10-06'
date-modified: today
date-format: long
format:
  html:
    theme: cerulean
    toc: true
    toc-depth: 2

execute:
  echo: true
  eval: true
  freeze: auto
---

# Load the dataset
```{python}
#echo: true
#eval: true
from pyspark.sql import SparkSession
import pandas as pd
import plotly.express as px
import plotly.io as pio
import numpy as np

np.random.seed(42)

pio.renderers.default = "notebook+notebook_connected+vscode"

# Initialize Spark Session
spark = SparkSession.builder.appName("LightcastData").getOrCreate()

# Load Data
df = spark.read.option("header", "true").option("inferSchema", "true").option("multiLine","true").option("escape", "\"").csv("./data/lightcast_job_postings.csv")

# Show Schema and Sample Data
print("---This is Diagnostic check, No need to print it in the final doc---")

# df.printSchema() # comment this line when rendering the submission
df.show(5)

```

# Feature Engineering
```{python}
#| eval: true
#| echo: true
#| fig-align: center

from pyspark.sql.functions import col, pow  # pow not used here; safe to keep
from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler
from pyspark.ml import Pipeline

# Columns to preview for quick EDA
eda_cols = [
    "SALARY",
    "MIN_YEARS_EXPERIENCE", "DURATION",
    "COMPANY_IS_STAFFING", "IS_INTERNSHIP",
    "STATE_NAME", "REMOTE_TYPE_NAME", "EMPLOYMENT_TYPE_NAME",
    "MIN_EDULEVELS_NAME"
]

# Keep only the columns that actually exist; warn about any missing
existing_cols = [c for c in eda_cols if c in df.columns]
missing_cols  = [c for c in eda_cols if c not in df.columns]
if missing_cols:
    print("⚠️ Missing columns (skipped):", missing_cols)

# Select and show a few rows
df_eda = df.select(*existing_cols)
print("Showing columns:", existing_cols)
df_eda.show(5, truncate=False)


```

# Empty columns plot
```{python}
# Missingness by column (nulls + blank strings) + Plotly bar chart

from pyspark.sql.functions import col, sum as spark_sum, when, trim, length
import pandas as pd
import plotly.express as px

# Use your EDA subset if it exists; otherwise use the full df
src = df_eda if 'df_eda' in globals() else df

# 1) Spark: count null OR blank-string cells per column
null_or_blank_sums = [
    spark_sum(
        when(col(c).isNull() | (length(trim(col(c).cast("string"))) == 0), 1).otherwise(0)
    ).alias(c)
    for c in src.columns
]
missing_df = src.select(null_or_blank_sums)

# 2) To pandas + percentages
missing_pd = missing_df.toPandas().T.reset_index()
missing_pd.columns = ["column", "missing_count"]
total_rows = src.count()
missing_pd["missing_pct"] = 100.0 * missing_pd["missing_count"] / max(total_rows, 1)
missing_pd = missing_pd.sort_values("missing_pct", ascending=False)

# 3) Plot
fig = px.bar(
    missing_pd,
    x="column",
    y="missing_pct",
    title="Percentage of Missing Values by Column",
    labels={"column": "Features", "missing_pct": "Missing (%)"}
)
fig.update_layout(xaxis_tickangle=-45, height=500, width=900)
fig


```

# Unique values
```{python}
from pyspark.sql.functions import countDistinct

# use df_eda if it exists; else fall back to df
src = df_eda if 'df_eda' in globals() else df

src.select([countDistinct(c).alias(f"{c}_nunique") for c in src.columns]).show(truncate=False)


```

# Category counter
```{python}
from pyspark.sql.functions import count, desc

# Use your EDA subset if available
src = df_eda if 'df_eda' in globals() else df

categorical_cols = [
    "STATE_NAME",
    "REMOTE_TYPE_NAME",
    "EMPLOYMENT_TYPE_NAME",
    "MIN_EDULEVELS_NAME",
    "COMPANY_IS_STAFFING",
    "IS_INTERNSHIP",
]

TOP_N = 50  # how many categories to show per column

for colname in categorical_cols:
    if colname not in src.columns:
        print(f"Skipping {colname} (column not found)")
        continue

    distinct_cnt = src.select(colname).distinct().count()
    print(f"\n--- {colname} (distinct={distinct_cnt}) ---")
    (
        src.groupBy(colname)
           .agg(count("*").alias("count"))
           .orderBy(desc("count"))
           .show(min(TOP_N, distinct_cnt), truncate=False)
    )


```

#  Remote Type Name Fix
```{python}
from pyspark.sql.functions import col, when, trim

# Start from df_eda if it exists; otherwise use df
base = df_eda if 'df_eda' in globals() else df

# Normalize REMOTE_TYPE_NAME values
df_eda = (
    base.withColumn(
        "REMOTE_TYPE_NAME",
        when(trim(col("REMOTE_TYPE_NAME")) == "Remote", "Remote")
        .when(trim(col("REMOTE_TYPE_NAME")) == "[None]", "Undefined")
        .when(trim(col("REMOTE_TYPE_NAME")) == "Not Remote", "On Premise")
        .when(trim(col("REMOTE_TYPE_NAME")) == "Hybrid Remote", "Hybrid")
        .when(col("REMOTE_TYPE_NAME").isNull(), "On Premise")
        .otherwise(col("REMOTE_TYPE_NAME"))
    )
)

# df_eda.createOrReplaceTempView("df_eda")  # ← uncomment if you want a SQL temp view

categorical_cols = [
   "REMOTE_TYPE_NAME",
    
]

for colname in categorical_cols:
    if colname not in df_eda.columns:
        print(f"⚠️ Skipping {colname} (column not found)")
        continue
    print(f"\n--- {colname} ---")
    df_eda.select(colname).distinct().show(50, truncate=False)


```

# Employment type fix
```{python}
# Normalize EMPLOYMENT_TYPE_NAME values and inspect them

from pyspark.sql.functions import col, when, trim, lower, regexp_replace, desc, count

# Start from df_eda if it exists; otherwise use df
base = df_eda if 'df_eda' in globals() else df

# A normalized helper (lowercased, trimmed, single-spaced) for matching
emp_norm = regexp_replace(lower(trim(col("EMPLOYMENT_TYPE_NAME"))), r"\s+", " ")

df_eda = (
    base.withColumn(
        "EMPLOYMENT_TYPE_NAME",
        when(col("EMPLOYMENT_TYPE_NAME").isNull(), "Fulltime")                                   # NULL → Fulltime
        .when(emp_norm.isin("part-time / full-time", "part time / full time", "part-time/full-time"),
              "Flexible")                                                                        # "Part-time / full-time" → Flexible
        .when(emp_norm.rlike(r"part[- ]?time\s*\((<=|≤)\s*32\s*hours\)"),
              "Parttime")                                                                        # "Part-time (<=/≤ 32 hours)" → Parttime
        .when(emp_norm.rlike(r"full[- ]?time\s*\(>\s*32\s*hours\)"),
              "Fulltime")                                                                        # "Full-time (> 32 hours)" → Fulltime
        .otherwise(col("EMPLOYMENT_TYPE_NAME"))                                                  # keep others as-is
    )
)

# --- Option A: just distinct values (like your screenshot) ---
print("\n--- EMPLOYMENT_TYPE_NAME (distinct) ---")
df_eda.select("EMPLOYMENT_TYPE_NAME").distinct().show(50, truncate=False)

# --- Option B: frequency table (more informative) ---
print("\n--- EMPLOYMENT_TYPE_NAME (counts) ---")
(df_eda.groupBy("EMPLOYMENT_TYPE_NAME")
       .agg(count("*").alias("count"))
       .orderBy(desc("count"))
       .show(truncate=False))

```
# Replace Company is Staffing with false, and is internship null with false
```{python}
# Replace NULLs with False in COMPANY_IS_STAFFING and IS_INTERNSHIP,
# then show distinct values (and counts)

from pyspark.sql.functions import col, coalesce, lit, count, desc

# start from df_eda if it exists; otherwise use df
base = df_eda if 'df_eda' in globals() else df

df_eda = (
    base
    .withColumn("COMPANY_IS_STAFFING", coalesce(col("COMPANY_IS_STAFFING"), lit(False)).cast("boolean"))
    .withColumn("IS_INTERNSHIP",       coalesce(col("IS_INTERNSHIP"),       lit(False)).cast("boolean"))
)

categorical_cols = ["COMPANY_IS_STAFFING", "IS_INTERNSHIP"]

for colname in categorical_cols:
    if colname not in df_eda.columns:
        print(f"⚠️ Skipping {colname} (not found)")
        continue
    print(f"\n--- {colname} (distinct) ---")
    df_eda.select(colname).distinct().show(10, truncate=False)

    print(f"--- {colname} (counts) ---")
    (df_eda.groupBy(colname)
           .agg(count("*").alias("count"))
           .orderBy(desc("count"))
           .show(truncate=False))


```

# Calculate median of the Duration column
```{python}
from pyspark.sql.functions import col, coalesce, lit

# use df_eda if it exists; otherwise start from df
df_eda = df_eda if 'df_eda' in globals() else df

# ensure DURATION is numeric for the quantile calc
df_eda = df_eda.withColumn("DURATION", col("DURATION").cast("double"))

# median via approxQuantile (ignore nulls)
median_duration = (
    df_eda.where(col("DURATION").isNotNull())
          .approxQuantile("DURATION", [0.5], 0.01)[0]
)

# fill nulls with the median (and keep as double)
df_eda = df_eda.withColumn(
    "DURATION",
    coalesce(col("DURATION"), lit(float(median_duration))).cast("double")
)

print(f"Median(DURATION) used for imputation: {median_duration}")
df_eda.select("DURATION").summary("count","min","50%","max").show()

```


# Value names
```{python}
df_eda.show(5, truncate=False)

```

# Feature Engineering
```{python}
from pyspark.sql.functions import col, pow
from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler
from pyspark.ml import Pipeline

# --- 1) Drop rows with NA in required columns (safe: only keep columns that exist) ---
required_cols = [
    "SALARY", "MIN_YEARS_EXPERIENCE", "STATE_NAME",
    "EMPLOYMENT_TYPE_NAME", "REMOTE_TYPE_NAME", "MIN_EDULEVELS_NAME",
    "DURATION", "IS_INTERNSHIP", "COMPANY_IS_STAFFING", "MAX_YEARS_EXPERIENCE"
]
keep_cols = [c for c in required_cols if c in df_eda.columns]
df_feature_eng = df_eda.dropna(subset=keep_cols)

# --- 2) Categorical columns (edit if you want fewer/more) ---
categorical_cols = [
    "STATE_NAME", "MIN_EDULEVELS_NAME",
    "EMPLOYMENT_TYPE_NAME", "REMOTE_TYPE_NAME",
    
]
categorical_cols = [c for c in categorical_cols if c in df_feature_eng.columns]

# --- 3) StringIndexer + OneHotEncoder ---
indexers = [
    StringIndexer(inputCol=c, outputCol=f"{c}_idx", handleInvalid="skip")
    for c in categorical_cols
]

encoder = OneHotEncoder(
    inputCols=[f"{c}_idx" for c in categorical_cols],
    outputCols=[f"{c}_vec" for c in categorical_cols],
    dropLast=True,         # avoid dummy-variable trap
    handleInvalid="keep"   # keep unseen levels at transform time
)

# --- 4) Assemble base features (for GLR / RF) ---
def present(cols):
    return [c for c in cols if c in df_feature_eng.columns]

numeric_base = present([
    "MIN_YEARS_EXPERIENCE", "MAX_YEARS_EXPERIENCE",
    "DURATION", "IS_INTERNSHIP", "COMPANY_IS_STAFFING" 
])

assembler = VectorAssembler(
    inputCols=numeric_base + [f"{c}_vec" for c in categorical_cols],
    outputCol="features"
)

# --- 5) Build pipeline and transform ---
pipeline = Pipeline(stages=indexers + [encoder, assembler])
data = pipeline.fit(df_feature_eng).transform(df_feature_eng)

# --- 6) Create squared term for polynomial regression ---
data = data.withColumn(
    "MIN_YEARS_EXPERIENCE_SQ",
    pow(col("MIN_YEARS_EXPERIENCE"), 2)
)

# --- 7) Assemble polynomial features vector ---
poly_inputs = present([
    "MIN_YEARS_EXPERIENCE", "MIN_YEARS_EXPERIENCE_SQ",
    "MAX_YEARS_EXPERIENCE", "DURATION", "IS_INTERNSHIP", "COMPANY_IS_STAFFING"
]) + [f"{c}_vec" for c in categorical_cols]

assembler_poly = VectorAssembler(
    inputCols=poly_inputs,
    outputCol="features_poly"
)
data = assembler_poly.transform(data)

# --- 8) Show final structure ---
data.select("SALARY", "features", "features_poly").show(5, truncate=False)



```
# Feature Engineering
```{python}
#| eval: true
#| echo: false
#| fig-align: center

from pyspark.sql.functions import col, pow
from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler
from pyspark.ml import Pipeline

# Columns you want to work with
eda_cols = [
    "SALARY",
    "MIN_YEARS_EXPERIENCE", "MAX_YEARS_EXPERIENCE", "DURATION",
    "COMPANY_IS_STAFFING", "IS_INTERNSHIP",
    "STATE_NAME", "REMOTE_TYPE_NAME", "EMPLOYMENT_TYPE_NAME",
    "MIN_EDULEVELS_NAME"
]

# Keep only columns that exist to avoid AnalysisException
existing_cols = [c for c in eda_cols if c in df.columns]
missing = [c for c in eda_cols if c not in df.columns]
if missing:
    print("⚠️ Skipping missing columns:", missing)

df = df.select(*existing_cols)

# Drop rows with NA in these key fields (use only those that are present)
subset_needed = [
    "SALARY", "MIN_YEARS_EXPERIENCE", "MAX_YEARS_EXPERIENCE",
    "EDUCATION_LEVELS_NAME", "EMPLOYMENT_TYPE_NAME", "REMOTE_TYPE_NAME",
    "DURATION", "IS_INTERNSHIP", "COMPANY_IS_STAFFING"
]
subset_cols = [c for c in subset_needed if c in df.columns]
df = df.dropna(subset=subset_cols)


```

# Indexer
```{python}
from pyspark.ml import Pipeline
from pyspark.ml.feature import StringIndexer

# Choose which categoricals to index (filter to those that exist)
categorical_cols = ["EMPLOYMENT_TYPE_NAME", "REMOTE_TYPE_NAME"]
categorical_cols = [c for c in categorical_cols if c in df.columns]

# One indexer per categorical
indexers = [
    StringIndexer(inputCol=c, outputCol=f"{c}_idx", handleInvalid="keep")
    for c in categorical_cols
]

# Fit + transform
pipeline = Pipeline(stages=indexers)
indexed_df = pipeline.fit(df).transform(df)

indexed_df.show(10, truncate=False)


```

YOU ARE HERE AT 2:27 IN VIDDEO